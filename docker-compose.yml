
services:
  postgres:
    image: postgres:14
    container_name: hospital_postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: hospital_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./csv:/csv
  pgadmin:
    image: dpage/pgadmin4
    container_name: hospital_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "8080:80"



  etl:
    build: ./script
    container_name: hospital_etl
    depends_on:
      - postgres
    volumes:
      - ./csv:/csv
      - ./script:/app
      - ./script/gcp_credentials.json:/app/gcp_credentials.json 

  dbt:
    build: ./dbt
    container_name: hospital_dbt
    volumes:
      - ./dbt:/usr/app
    entrypoint: ["/bin/bash"]      
    command: ["-c", "tail -f /dev/null"]
     # Airflow Services
  airflow_postgres:
    image: postgres:14
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5433:5432"
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 10
      start_period: 30s


  airflow_init:
    image: apache/airflow:2.7.3-python3.11
    container_name: airflow_init
    entrypoint: /bin/bash
    command: -c "
      echo 'Airflow initialization...';
      mkdir -p /opt/airflow/dags /opt/airflow/plugins /opt/airflow/logs;
      airflow db migrate;

      airflow users create \
        -u admin -p admin -f admin -l user -r Admin -e admin@example.com || true; "
    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres:5432/airflow
      
      AIRFLOW_UID: 50000
      AIRFLOW_GID: 0
    volumes:
      - ./airflow:/opt/airflow/dags
      - ./script:/opt/airflow/scripts
      - ./dbt:/opt/airflow/dbt_project
      - ./script/gcp_credentials.json:/opt/airflow/gcp_credentials.json
    depends_on:
      airflow_postgres:
        condition: service_healthy


  airflow_webserver:
    image: apache/airflow:2.7.3-python3.11
    container_name: airflow_webserver
    entrypoint: ["bash", "-c", "airflow webserver"]
    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres:5432/airflow
      
      AIRFLOW_UID: 50000
      AIRFLOW_GID: 0
    
    ports:
      - "8081:8080"
    volumes:
      - ./airflow:/opt/airflow/dags
      - ./script:/opt/airflow/scripts
      - ./dbt:/opt/airflow/dbt_project
      - ./script/gcp_credentials.json:/opt/airflow/gcp_credentials.json
    restart: always
    depends_on:
      airflow_postgres:
        condition: service_healthy
      airflow_init:
        condition: service_completed_successfully

  airflow_scheduler:
    image: apache/airflow:2.7.3-python3.11
    container_name: airflow_scheduler
    entrypoint: ["bash", "-c", "airflow scheduler"]
    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres:5432/airflow
      
      AIRFLOW_UID: 50000
      AIRFLOW_GID: 0
    
    volumes:
      - ./airflow:/opt/airflow/dags
      - ./script:/opt/airflow/scripts
      - ./dbt:/opt/airflow/dbt_project
      - ./script/gcp_credentials.json:/opt/airflow/gcp_credentials.json
    restart: always
    depends_on:
      airflow_postgres:
        condition: service_healthy
      airflow_init:
        condition: service_completed_successfully

volumes:
  postgres_data:
  airflow_db_data:
